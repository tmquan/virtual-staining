hydra:
  output_subdir: null
  run:
    dir: .

defaults:
  - override hydra/hydra_logging: disabled
  - override hydra/job_logging: disabled

data:
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/P250/20X/no_paraffin_uncoverslip_BF_20X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/P250/20X/no_paraffin_glycerol_coverslip_BF_20X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/P250/20X/no_paraffin_coverslip_BF_20X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/P250/20X/paraffin_uncoverslip_BF_20X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/20X/no_paraffin_uncoverslip_BF_20X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/20X/no_paraffin_glycerol_coverslip_BF_20X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/20X/no_paraffin_coverslip_BF_20X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/20X/paraffin_uncoverslip_BF_20X"
  root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/40X/no_paraffin_uncoverslip_BF_40X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/40X/no_paraffin_glycerol_coverslip_BF_40X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/40X/no_paraffin_coverslip_BF_40X"
  # root_folder: "data/APPROACH_EyePaint_pix2pixHD/STEP1_Preprocessing/ACT2_EyePaint/Slide1/NanoZoomer/40X/paraffin_uncoverslip_BF_40X"
  train_samples: 2000
  val_samples: 400
  test_samples: 1000
  img_shape: 256
  vol_shape: 256
  batch_size: 4

model:
  phase: "pretrain"
  timesteps: 1000
  prediction_type: "sample" # "sample" or "epsilon" or "v_prediction"
  img_shape: ${data.img_shape}
  vol_shape: ${data.vol_shape}
  batch_size: ${data.batch_size}

train:
  ckpt: ${resume_from_checkpoint}
  strict: 1 if ${resume_from_checkpoint} is not None else 0
  lr: 2e-5
  alpha: 1
  gamma: 1
  perceptual: false
  lamda: 2e-3
  batch_size: ${data.batch_size}
  epochs: 300
  ema_decay: 0.9999 # `-1` disables it


resume_from_checkpoint: logs/pretrain/version_1/checkpoints/epoch=199-step=50000.ckpt
test: true


trainer:
  accelerator: auto
  devices: 1
  precision: 32
  strategy: auto
  max_epochs: ${train.epochs}
  enable_model_summary: true
  # amp_backend: apex

callbacks:
  - _target_: lightning.pytorch.callbacks.ModelCheckpoint
    monitor: "validation_loss_epoch"
    auto_insert_metric_name: true
    save_top_k: -1
    save_last: true
    every_n_epochs: 20
  - _target_: lightning.pytorch.callbacks.LearningRateMonitor
    logging_interval: 'epoch'
    log_momentum: true
    log_weight_decay: true
  - _target_: lightning.pytorch.callbacks.RichProgressBar
    refresh_rate: 1
  - _target_: lightning.pytorch.callbacks.StochasticWeightAveraging
    swa_lrs: 1e-3  

logger:
  - _target_: lightning.pytorch.loggers.TensorBoardLogger
    save_dir: "./logs_test"
    log_graph: true
    name: ${model.phase}
